# PathFinder-Ship

PathFinder-Ship is a local-first, intent-driven multimodal assistant designed to run entirely on CPU. It integrates an ONNX-deployed Flan-T5 Large encoderâ€“decoder model for instruction-following responses (783M parameters; referred to as **Passenger-Bot** in this project). The Passenger-Bot checkpoint was additionally instruction fine-tuned in a multi-task setup to jointly support chat and RAG (retrieval-augmented generation) workflows. 

The system further combines: a quantized MiniLM intent classifier for command routing and conversational steering; a hybrid RAG engine with ChromaDB + SQLite FTS5/BM25 and score-based web-search gating; and a YOLO-NAS vision pipeline that can open the camera, capture photos, run object detection, annotate frames, and email results.

All capabilities are delivered through a single FastAPI backend and a modern single-page web UI featuring chat history, file upload, web-search toggling, and an optional voice mode. 

There are no canned repliesâ€”every message is generated by the model under instruction guidance.

---

## Table of Contents

- [Demo](#demo)
- [Key Features](#key-features)
- [Architecture Overview](#architecture-overview)
- [Project Structure](#project-structure)
- [Getting Started](#getting-started)
  - [1. Clone & Environment](#1-clone--environment)
  - [2. Environment Variables](#2-environment-variables)
  - [3. Prepare RAG Corpus and Build Index](#3-prepare-rag-corpus-and-build-index)
  - [4. Run Backend and Frontend](#4-run-backend-and-frontend)
- [How to Use](#how-to-use)
  - [Chat](#chat)
  - [RAG QA over Your Documents](#rag-qa-over-your-documents)
  - [Web-Augmented Answers](#web-augmented-answers)
  - [Camera, Photos and Object Detection](#camera-photos-and-object-detection)
  - [Voice Mode](#voice-mode)
- [Internals](#internals)
  - [Intent Classifier (MiniLM ONNX)](#intent-classifier-minilm-onnx)
  - [Flan-T5 ONNX Service](#flan-t5-onnx-service)
  - [Hybrid RAG Engine](#hybrid-rag-engine)
  - [Web Search Integration](#web-search-integration)
  - [Vision Pipeline and Email Notifications](#vision-pipeline-and-email-notifications)
- [Use Cases](#use-cases)
- [Limitations and Roadmap](#limitations-and-roadmap)
- [Credits](#credits)
- [Additional Information](#additional-information)

---

## Demo

> ğŸ”— **Demo video** 
ğŸ¥ [Watch on YouTube](https://youtu.be/mqfz_hPWoi0)

---

## Key Features

- **Intent routing with MiniLM (ONNX INT8)**  
  - Classifies every user message into one of:  
    `open_camera`, `close_camera`, `take_photo`, `object_detect`, `chat`.
  - High-confidence commands go directly to camera / vision tools.
  - Other messages flow into chat or RAG.

- **Hybrid RAG over your own documents**
  - SentenceTransformer (`all-MiniLM-L6-v2`) embeddings stored in **ChromaDB**.
  - The same chunks indexed in **SQLite FTS5 (BM25)**.
  - At query time, semantic and keyword signals are normalized and combined into a single hybrid relevance score in `[0, 1]`.

- **Optional web-augmented answers**
  - DuckDuckGo-based web search (via `ddgs`).
  - HTML cleaning, chunking, and simple relevance scoring.
  - Strict â€œweb strengthâ€ gating: web chunks are used only when strong enough to be trusted.

- **Flan-T5 Large (ONNX INT8) with instruction prompts**
  - One model, multiple behaviors:
    - free-form chat,
    - RAG QA grounded in retrieved context,
    - safe fallback answers when RAG is weak,
    - narrations for camera actions and detection results.

- **Vision tools with YOLO-NAS**
  - Open the camera in the browser.
  - Take a snapshot.
  - Run object detection on:
    - a live frame from the camera, or
    - an uploaded image.
  - Draw bounding boxes and class labels.
  - Manage images with a ring buffer (only last N images kept).
  - Send detection/photo results to your email as attachments.

- **Modern single-page web UI**
  - Chat interface with history sidebar and localStorage persistence.
  - Markdown rendering and syntax-highlighted code blocks.
  - File upload and detection on uploaded images.
  - Web search toggle (local only vs local+web).
  - Theme toggle (light/dark).
  - Voice mode (browser STT + TTS, English only).

All core models are ONNX INT8 and run locally on **CPU** via ONNX Runtime.

---

## Architecture Overview

High-level request flow:

1. **Frontend** sends user input to `POST /api/intent`.
2. **Intent classifier (MiniLM ONNX)** returns `(intent, score)`:
   - `open_camera`, `close_camera`, `take_photo`, `object_detect`, `chat`.
3. If `intent` is a **command** and `score >= CLS_ROUTE_THRESHOLD`:
   - Camera and vision actions are executed in the browser + backend:
     - open/close camera,
     - capture photo,
     - run YOLO detection,
     - store image and optionally send via email.
   - Flan-T5 generates short narrations confirming the action.
4. Otherwise, message is treated as **chat / QA**:
   - If user toggled **web search OFF**:
     - RAG over local documents, or
     - pure model-only answer with a safe instruction if RAG is weak.
   - If **web search ON**:
     - Web results are fetched and scored.
     - A web strength score decides whether to use web chunks.
     - Local + web, web-only, or model-only is selected via a score-based decision tree.
5. Flan-T5 produces the final answer based on:
   - a chat instruction (for plain conversation),
   - a strict RAG instruction (for grounded QA),
   - or a fallback instruction (for low-confidence situations).

A single FastAPI backend orchestrates:

- `NLUClassifier` (MiniLM),
- `T5Service` (Flan-T5),
- `RAGService` (Chroma + BM25 + web),
- `YOLOService` (YOLO-NAS),
- and utility layers for storage, mail, and prompt building.

![1758439761461](https://github.com/user-attachments/assets/d561f507-6d75-48ca-9a1f-c3032491ae3d)

![1758439761198](https://github.com/user-attachments/assets/1eb601bb-4e4a-443d-9668-44425c7f8aa2)

---

## Project Structure

```text
PathFinder-Ship/
â”œâ”€ backend/
â”‚  â”œâ”€ main.py                 # FastAPI / uvicorn entrypoint
â”‚  â”œâ”€ config.py               # .env â†’ CFG dict (paths, thresholds, model locations)
â”‚  â”œâ”€ web/
â”‚  â”‚  â””â”€ app.py               # API routes: /api/intent, /api/chat, /api/rag, /api/photo, /api/detect, /api/upload, /api/health
â”‚  â”œâ”€ assets/
â”‚  â”‚  â”œâ”€ models/
â”‚  â”‚  â”‚  â”œâ”€ nlu/              # MiniLM intent ONNX + tokenizer
â”‚  â”‚  â”‚  â”œâ”€ t5/               # Flan-T5 ONNX encoder/decoder + tokenizer
â”‚  â”‚  â”‚  â””â”€ yolo_nas/         # YOLO-NAS ONNX + labels.txt
â”‚  â”‚  â””â”€ rag/
â”‚  â”‚     â””â”€ chroma_db/        # ChromaDB persistent store + BM25 SQLite
â”‚  â”œâ”€ data/
â”‚  â”‚  â”œâ”€ rag/
â”‚  â”‚  â”‚  â””â”€ corpus/           # PDF/DOC/TXT documents for RAG
â”‚  â”‚  â””â”€ web_out/
â”‚  â”‚     â”œâ”€ photo/            # ring-buffer photo outputs
â”‚  â”‚     â””â”€ detect/           # ring-buffer detection outputs
â”‚  â”œâ”€ services/
â”‚  â”‚  â”œâ”€ nlu_classifier.py    # MiniLM intent classifier wrapper
â”‚  â”‚  â”œâ”€ t5.py                # Flan-T5 ONNX (chat, RAG, narrations)
â”‚  â”‚  â”œâ”€ rag.py               # RAGService (hybrid search + web gating + context building)
â”‚  â”‚  â”œâ”€ yolo.py              # YOLO-NAS ONNX wrapper (pre/post, NMS)
â”‚  â”‚  â””â”€ rag_backend/
â”‚  â”‚     â”œâ”€ io_loader.py      # load PDF/DOC/TXT â†’ raw text
â”‚  â”‚     â”œâ”€ preprocess.py     # clean & chunk text (token-aware)
â”‚  â”‚     â”œâ”€ indexer.py        # SentenceTransformer embeddings + Chroma + FTS5 index
â”‚  â”‚     â”œâ”€ search.py         # hybrid retrieval (Chroma + BM25)
â”‚  â”‚     â”œâ”€ prompt.py         # tokenizer-aware context construction
â”‚  â”‚     â””â”€ websearch.py      # DuckDuckGo search + HTML parsing + web chunk scoring
â”‚  â””â”€ utils/
â”‚     â”œâ”€ text.py              # instructions and prompt builders for T5
â”‚     â”œâ”€ vision.py            # YOLO NMS + drawing utilities
â”‚     â”œâ”€ storage.py           # ring-buffer saving for photo/detect outputs
â”‚     â””â”€ mailer.py            # SMTP email helper for sending images
â”œâ”€ frontend/
â”‚  â”œâ”€ index.html              # SPA shell (sidebar + chat + input area + camera)
â”‚  â”œâ”€ styles.css              # modern, responsive styling (light/dark)
â”‚  â””â”€ app.js                  # chat logic, intent routing, camera, upload, voice mode
â””â”€ requirements.txt
```

---

## Getting Started

### 1. Clone & Environment

```bash
git clone https://github.com/YOUR_USERNAME/pathfinder-ship.git
cd pathfinder-ship

python -m venv .venv

# Windows (PowerShell)
Set-ExecutionPolicy -Scope Process -ExecutionPolicy RemoteSigned
.\.venv\Scripts\Activate.ps1

# Linux / macOS
source .venv/bin/activate

pip install --upgrade pip
pip install -r requirements.txt
```

### 2. Environment Variables

Create a file `backend/.env` based on the template below:

```env
# App
APP_NAME=PathFinder-Ship
DEFAULT_USER_NAME=Passenger
BOT_NAME=Passenger-Bot
DEBUG=false

# API
API_HOST=0.0.0.0
API_PORT=8000
FRONTEND_ORIGIN=http://localhost:5173

# Classifier (MiniLM-L6, ONNX INT8)
CLS_ONNX=assets/models/nlu/intent-minilm-int8.onnx
CLS_TOKENIZER_DIR=assets/models/nlu/tokenizer
CLS_MAX_LEN=64
CLS_ROUTE_THRESHOLD=0.50

# Storage (ring buffer)
PHOTO_DIR=data/web_out/photo
DETECT_DIR=data/web_out/detect
MAX_FILES_PER_DIR=10

# Email (for photo/detect notifications)
EMAIL_SMTP_HOST=smtp.gmail.com
EMAIL_SMTP_PORT=587
EMAIL_USE_TLS=1
EMAIL_USER=your_app_email@gmail.com
EMAIL_PASSWORD=your_app_password_here
EMAIL_FROM=PathFinder-Ship <your_app_email@gmail.com>
EMAIL_TO_PHONE=destination_email@example.com

# Flan-T5 Large (ONNX INT8)
T5_TOKENIZER_DIR=assets/models/t5/tokenizer
T5_ENCODER=assets/models/t5/encoder_model_int8.onnx
T5_DECODER=assets/models/t5/decoder_model_int8.onnx
T5_MAX_SRC_LEN=512
T5_MAX_NEW_TOKENS_CHAT=256
T5_MAX_NEW_TOKENS_RAG=64

# RAG
RAG_SCORE_THRESHOLD=0.40
RAG_TOP_K=4
RAG_MAX_CTX_TOKENS=512
EMBED_MODEL=sentence-transformers/all-MiniLM-L6-v2
CHROMA_PATH=assets/rag/chroma_db
VECTOR_WEIGHT=0.75
BM25_WEIGHT=0.25
RAG_WEB_MIN_STRENGTH=0.75
WEB_CHUNK_SUPPORT_THRESHOLD=0.70

# YOLO-NAS
YOLO_ONNX=assets/models/yolo_nas/yolo_nas_s_coco.onnx
YOLO_LABELS=assets/models/yolo_nas/labels.txt
YOLO_SIZE=640
YOLO_CONF=0.50
YOLO_IOU=0.50
```

### 3. Prepare RAG Corpus and Build Index

1. Put your documents into the RAG corpus folder:

   ```text
   backend/data/rag/corpus/
   ```

   Supported formats:
   - `.pdf`
   - `.docx`
   - `.txt`

2. From `backend/`, build or rebuild the index:

   ```bash
   cd backend
   python -m services.rag_backend.indexer --src data/rag/corpus --reset
   ```

This will:

- read documents (PDF/DOCX/TXT),
- clean and chunk them (token-aware, with overlap),
- embed chunks with `all-MiniLM-L6-v2`,
- write vectors into ChromaDB,
- index text into SQLite FTS5 (BM25).

### 4. Run Backend and Frontend

From `backend/`:

```bash
cd backend
python main.py
# FastAPI: http://0.0.0.0:8000
```

From `frontend/`:

```bash
cd frontend
python -m http.server 5173
```

Then open the web UI:

```text
http://localhost:5173
```

---

## How to Use

### Chat

- Type a message in the input area and hit **Send**.
- For small-talk (e.g. â€œWho are you?â€, â€œTell me a storyâ€), the system:
  - classifies intent as `chat`,
  - calls `POST /api/chat`,
  - Flan-T5 uses a chat instruction and returns a natural English answer.

### RAG QA over Your Documents

- Ask questions about the content in your RAG corpus, such as:
  - â€œSummarize the main findings of the 2014 Kosovo report.â€
  - â€œWhat does the thesis say about migration patterns?â€
- Backend flow:
  - `POST /api/rag` with `use_internet=false`.
  - Hybrid retrieval over Chroma + BM25.
  - If best score â‰¥ `RAG_SCORE_THRESHOLD`:
    - context is constructed with a strict RAG instruction and passed to T5.
  - If score is below threshold:
    - T5 is called with a safe fallback instruction and **no context**, encouraging it to avoid making up facts.

### Web-Augmented Answers

- Turn on the **Web Search** toggle in the UI.
- Ask open-domain questions (history, facts, etc.).
- Backend flow:
  - RAG over local corpus as usual.
  - Web results fetched using DuckDuckGo, cleaned and chunked.
  - A `web_strength` score decides whether web chunks are eligible.
  - Decision matrix:
    - strong local + strong web â†’ hybrid local+web context,
    - strong local, weak web â†’ local-only context,
    - weak local, strong web â†’ web-only context,
    - both weak â†’ model-only fallback.

The backend returns `used_context` and `sources`, and the UI can render a â€œResourcesâ€ section under the answer.

### Camera, Photos and Object Detection

Typical flows:

1. **Open camera**

   - Type: â€œOpen the cameraâ€ / â€œTurn on cameraâ€.
   - Intent classifier returns `open_camera` with high score.
   - Frontend opens `navigator.mediaDevices.getUserMedia` video stream.
   - Backend uses T5 to generate a short narration (â€œOpening the camera for you now...â€).
   - Chat shows a confirmation plus the live video preview.

2. **Take a photo**

   - Type: â€œTake a photoâ€.
   - The browser captures the current frame and sends it to `POST /api/photo`.
   - Backend:
     - saves it to `PHOTO_DIR` using a ring buffer (`photo_01.jpg`, `photo_02.jpg`, â€¦),
     - triggers a background task that emails the image to `EMAIL_TO_PHONE`.
   - Response includes a static URL pointing to the stored image.

![detect_10](https://github.com/user-attachments/assets/56f46598-fc34-4b9d-a6d7-ec025bb7fdb8)


3. **Run object detection**

   Two options:

   - **From camera**:
     - Open camera first.
     - Type: â€œObject detectâ€ / â€œDetect objectsâ€.
     - Last frame is captured, sent to `POST /api/detect`.
   - **From uploaded image**:
     - Click the file upload button and choose an image.
     - Send a message; frontend calls `POST /api/detect` with that image.

   Backend detection flow:

   - YOLO-NAS ONNX runs on the image.
   - Bounding boxes and labels are created.
   - A summary string is built (e.g. `"2 person, 1 cell phone"`).
   - T5 generates a short narrative based on the summary:
     - high-level guess of the scene,
     - careful wording (no strong claims, avoids â€œI detected â€¦â€ phrasing).
   - A drawn copy of the image is saved to `DETECT_DIR` via ring buffer.
   - Both the image and the narrative+summary are emailed in the background.
   - API response includes:
     - `labels`, `summary`, `narration`, and a static `image_url`.

### Voice Mode

- Voice mode uses the browserâ€™s Web Speech API:
  - STT (speech-to-text) to capture your question,
  - TTS (text-to-speech) to read the answer.
- When **Voice** is toggled:
  - The text input is disabled.
  - Each transcript is sent to `POST /api/rag` with:
    - `use_internet` and `web_only` derived from the Web Search toggle.
  - The answer is spoken back and can also be rendered in the chat.

> Note: Voice mode currently focuses on Q&A (RAG + web).  
> It does not yet pass through intent routing to open the camera or run detection via voice commands.

---

## Internals

### Intent Classifier (MiniLM ONNX)

- Quantized MiniLM-L6 model exported to ONNX (INT8) for CPU.
- Uses a local HuggingFace tokenizer and config.
- Returns a canonical label and a softmax probability.
- A global `CLS_ROUTE_THRESHOLD` controls when a command is â€œconfident enoughâ€ to bypass chat and trigger camera tools.

### Flan-T5 ONNX Service

- Encoder and decoder are exported as separate ONNX models.
- Tokenizer is loaded locally (`assets/models/t5/tokenizer`).
- Two main generation modes:
  - **chat**: sampling (top-p, temperature) for natural conversation,
  - **rag**: greedy, shorter sequences for focused answers.
- Multiple prompt templates:
  - `chat_instruction`: defines the assistant personality and context.
  - `rag_instruction`: enforces strict grounding to context.
  - `fallback_instruction`: used when no reliable context is available.
  - Camera-specific instructions for open/close/take-photo and detection narrations.

### Hybrid RAG Engine

- `rag_backend.preprocess`:
  - cleans text (removes soft hyphens, normalizes whitespace),
  - chunks documents via token-based or word-based strategy.
- `rag_backend.indexer`:
  - encodes chunks with `all-MiniLM-L6-v2`,
  - stores embeddings in a persistent Chroma collection,
  - stores plain text + metadata in SQLite FTS5.
- `rag_backend.search`:
  - `chroma_search`: semantic similarity via distances â†’ similarities â†’ normalization.
  - `bm25_search`: keyword search via FTS5 BM25 (scores inverted and normalized).
  - `hybrid_search`: merges both into a unified score in `[0, 1]` using `VECTOR_WEIGHT` and `BM25_WEIGHT`.

- `RAGService`:
  - wraps `hybrid_search` and `create_context`,
  - returns:
    - the context list,
    - the best local score,
    - and a list of `sources` (`local:<file>` and/or web URLs),
  - decides whether to use:
    - local-only,
    - web-only,
    - hybrid local+web,
    - or model-only fallback.

<img width="1919" height="1079" alt="Ekran gÃ¶rÃ¼ntÃ¼sÃ¼ 2025-10-01 201021" src="https://github.com/user-attachments/assets/3598dfdb-fe97-416f-adb6-fbe6d9dd30ff" />


### Web Search Integration

- Uses DuckDuckGo via `ddgs` for search results.
- Scrapes pages using `requests` + `BeautifulSoup` with a conservative user agent.
- Removes scripts/styles, normalizes text, and chunks content.
- Each chunk receives a simple relevance score based on query token overlap.
- A `web_strength` function compresses:
  - the top chunk score, and
  - the number of strong chunks,
  into a single scalar in `[0, 1]`.
- Web chunks are only considered if `web_strength >= RAG_WEB_MIN_STRENGTH`.

### Vision Pipeline and Email Notifications

- YOLO-NAS ONNX export is used with ONNX Runtime on CPU.
- Preprocessing:
  - letterbox resize,
  - BGR â†’ RGB,
  - scale to `[0, 1]`,
  - NCHW blob.
- Postprocessing:
  - ensures correct shape for boxes and scores,
  - picks class IDs and confidences,
  - de-letterboxes boxes back to original coordinates,
  - clips to image bounds,
  - applies NMS,
  - returns final detections `(x1, y1, x2, y2, conf, cls_id)`.
- Drawing:
  - rectangles and labels are drawn using OpenCV.
  - colors and fonts are fixed to avoid complexity.
- Storage:
  - `utils.storage.save_with_ring_buffer` maintains a rolling index (`.ring.idx`) in each folder,
  - files are named `photo_01.jpg`, `detect_01.jpg` â€¦ up to `MAX_FILES_PER_DIR`.
- Email:
  - `utils.mailer.send_image_via_email` sends a single image with subject/body via SMTP,
  - skips sending if required config is missing (safe fallback).

---

## Use Cases

### 1. Personal Knowledge Base Copilot

- Maintain your study or work documents as PDFs/DOCX/TXT in the RAG corpus.
- Ask focused questions about these documents.
- The assistant:
  - retrieves only relevant chunks,
  - answers strictly from those chunks when possible,
  - otherwise clearly falls back to â€œI donâ€™t know / not in the documents.â€

### 2. Visual Incident Logger / Desk Assistant

- Keep a webcam connected to your machine.
- Use commands such as:
  - â€œOpen cameraâ€,
  - â€œTake a photoâ€,
  - â€œObject detectâ€.
- The system:
  - activates the camera in the browser,
  - captures frames on demand,
  - runs YOLO-NAS object detection,
  - annotates and saves images in a ring buffer,
  - sends detection results (image + text) to your inbox.

### 3. Edge/Embedded AI Console

- Because all models run on CPU via ONNX Runtime, the same backend can be:
  - ported to Jetson or industrial PCs,
  - adapted to different camera sources,
  - connected to domain-specific corpora or private web endpoints.
- The modular service structure (`services/*.py`) makes it straightforward to:
  - swap model weights,
  - change thresholds,
  - or plug in new tools (e.g., ASR, OCR).

---

## Limitations and Roadmap

- **Visual descriptions can hallucinate details**  
  - The detection narrative is based on YOLO labels and context, not on pixel-perfect reasoning.
  - It may sometimes invent colors or clothing details.
  - For safety-critical scenarios, rely on the raw detection summary, not the narrative sentence.

- **Latency on low-powered CPUs**  
  - Flan-T5 Large, even in INT8, is heavy.
  - Long chat responses may be slow on low-end CPUs.
  - Possible future work: switch to smaller encoder-decoder models or use distillation.

- **Voice mode is RAG-only**  
  - Voice input currently goes directly to `/api/rag`.
  - It does not yet trigger camera commands via intent classification.
  - Future work: route voice transcripts through the same intent pipeline as text.

- **Single-user, local prototype**  
  - No authentication or multi-tenant separation.
  - For production, you would add:
    - user accounts,
    - per-user corpora,
    - logging, monitoring, and rate limiting.

---

## Credits

- **Flan-T5 Large** â€“ Google / HuggingFace (converted to ONNX INT8).
- **MiniLM-L6** â€“ SentenceTransformers / HuggingFace (intent classifier + embeddings).
- **ChromaDB** â€“ vector database for embeddings.
- **SQLite FTS5** â€“ BM25 keyword search.
- **YOLO-NAS** â€“ Deci / SuperGradients object detector (exported to ONNX).
- **DuckDuckGo (ddgs)** â€“ web search backend.
- **FastAPI** â€“ backend framework.
- **Vanilla HTML/CSS/JS** â€“ frontend stack (with highlight.js and marked.js).

PathFinder-Ship combines intent routing, hybrid RAG, and object detection pipelines within a single system design. The goal is to provide both a "real system architecture" visible to companies and a practical, fully local assistant.

---

## Additional Information

* **Developer**: [Fatih AYIBASAN] (Computer Engineering Student)
* **Email**: [fathaybasn@gmail.com]

---
